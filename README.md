# [REINFORCE paper](https://arxiv.org/pdf/2402.14740) implementation

## Task 

RLHF [1] — фундаментальный метод алаймента языковых моделей, который применялся при обучении ChatGPT, LLama 2, Qwen 2.5 и т.д. Как известно RLHF очень требователен к ресурсам и чувствителен к гиперпараметрам. Сложность данного метода обоснована сложностью алгоритма PPO [2], который лежит в основе RLHF.


Статья Back to Basics [3] предлагает использовать более простой алгоритм REINFORCE для алаймента языковых моделей. Согласно результатам данной работы, данный алгоритм не только проще в реализации, но и показывает лучшие метрики, чем PPO.

### Level 1

Мы предлагаем вам реализовать алгоритм REINFORCE w/ baseline для алаймента языковых моделей. Обращаем ваше внимание, что вам не нужно реализовывать RLOO для подсчёта baseline, достаточно взять moving average.

- Используйте HuggingFaceTB/SmolLM2-135M-Instruct в качестве SFT модели.

- Поверх SFT обучите Reward Model на парах из датасета esfrankel17/HelpSteer2_binarized. Разбейте average_rting_split на train и validation подвыборки. На данном этапе рекомендуется использовать RewardTrainer из. библиотеки trl. Достаточно обучить одну эпоху. Используйте learning rate = 5e-5, fp16 и ограничьте максимальную длину так, чтобы избежать Out of Memory.
Рекомендуем куда-нибудь сохранить полученную RM.

- Реализуйте алгоритм REINFORCE w/ baseline из статьи, используя SFT и RM, полученные на предыдущих шагах. На данном этапе НЕ рекомендуется использовать RLOOTrainer из. библиотеки trl, так как реализация в нём отличается от алгоритма, описанного в статье (Bonus: в отчёте опишите почему именно). Используйте batch size и количество итераций, которые позволяют ваши ресурсы. В качестве baseline используйте moving average.

- Выросла ли средняя награда на отложенной выборке (validation split) по сравнению c SFT моделью?


### Level 2


Предположим, что мы хотим обучить Reward Model, которая выдаёт не скалярную оценку, а распределение вероятности поверх дискретных оценок. Пусть оценка текста — натуральное число от 1 до 10, тогда RM выдаёт 10 чисел, каждое из которых — вероятность текста получить соответствующую оценку (соответственно сумма этих значений равна 1).

- Подумайте, как должна выглядеть функция потерь для обучения такой модели наград, если мы по-прежнему хотим максимизировать вероятность $p(y_w \succ y_l | x)$.

- Обучите Reward Model с полученной функцией потерь на том же датасете пар.

- Придумайте, как данную модель интегрировать в алгоритм REINFORCE. Какую дополнительную информацию мы можем использовать, если работаем с распределением над оценками? Как этот сигнал интерпретируется?
Обучите REINFORCE поверх SFT с вероятностной RM. 

- Используйте те же гиперпараметры, что использовали при обучении в Level 1.

- Получилось ли улучшить качество алаймента? Как вы объясните полученный результат?


### Правила:

- Проанализируйте полученные результаты. Это самый важный пункт, потому что хочется увидеть не только числа с полученными метриками. Как вы объясняете увиденное поведение? Напишите отчет о проведенных экспериментах. Что получилось? Что нет?

- Нет правильного способа решить задачу. Не стоит беспокоиться, что вы делаете что-то неправильно. Мы хотим увидеть ваши способности к исследованиям, а не какое-то конкретное решение задачи. Возможно, вы придумаете то, о чем мы даже не задумывались изначально – это будет высший класс.

- Убедитесь, что результатам можно доверять. Исключите вариант случайности, etc.

- Вы можете использовать Google Colab или Kaggle Code, чтобы получить доступ к бесплатным вычислительным ресурсам.

- Присылайте решение в виде репозитория на github с отчетом по решению и чёткими инструкциями, как запустить ваш код. Убедитесь, что мы сможем запустить ваше решение по этим инструкциям.

- Вы можете использовать любые библиотеки и фреймворки, которые вам могут быть необходимы.

- Сфокусируйтесь на том, чтобы код был чист и понятен. Если вы считаете, что какая-то его часть может быть непонятна, то добавьте комментарии. Мы очень сильно ценим хорошо написанный код, поэтому если решение задачи будет оформлено грязно, то мы можем отклонить заявку.


### References

[1] Ouyang et al, Training language models to follow instructions with human feedback

[2] Schulman et al, Proximal Policy Optimization Algorithms

[3] Ahmadian et al, Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs