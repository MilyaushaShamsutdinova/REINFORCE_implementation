{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "seed = 28\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model to check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "# model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages = [{\"role\": \"user\", \"content\": \"What is gravity?\"}]\n",
    "# input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "# outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "# print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "m:\\python_projects\\REINFORCE_implementation\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"esfrankel17/HelpSteer2_binarized\", split='average_rating_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'chosen_rating', 'rejected', 'rejected_rating'],\n",
       "    num_rows: 8678\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'chosen_rating', 'rejected', 'rejected_rating'],\n",
       "        num_rows: 6942\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'chosen_rating', 'rejected', 'rejected_rating'],\n",
       "        num_rows: 1736\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What is the point group of m3m? any crystal group get this symmetry? Since it is a cubic point group, referred to Oh? is C4 also a cubic point group?\\n<extra_id_1>Assistant\\nThe point group of m3m is m3m, which has order 16 and is a cubic point group. The crystal structure of m3m is cubic, and the symmetry of the m3m group is referred to as Oh. The point group C4 also has order 16 and is a cubic point group, and has a cubic crystal structure with symmetry referred to as Ci.\\n<extra_id_1>User\\nare m3m and C4 describing the same structure?\\n<extra_id_1>Assistant\\nYes, m3m and C4 describe the same crystal structure. This structure has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.\\n<extra_id_1>User\\nGenerate a picture for me ?',\n",
       " 'chosen': [{'content': 'What is the point group of m3m? any crystal group get this symmetry? Since it is a cubic point group, referred to Oh? is C4 also a cubic point group?\\n<extra_id_1>Assistant\\nThe point group of m3m is m3m, which has order 16 and is a cubic point group. The crystal structure of m3m is cubic, and the symmetry of the m3m group is referred to as Oh. The point group C4 also has order 16 and is a cubic point group, and has a cubic crystal structure with symmetry referred to as Ci.\\n<extra_id_1>User\\nare m3m and C4 describing the same structure?\\n<extra_id_1>Assistant\\nYes, m3m and C4 describe the same crystal structure. This structure has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.\\n<extra_id_1>User\\nGenerate a picture for me ?',\n",
       "   'role': 'user'},\n",
       "  {'content': \"I'm sorry, as an AI language model, I don't have the capability to generate pictures or images. However, I can provide you with a description of what a m3m or C4 crystal structure looks like:\\n \\n\\n m3m or C4 crystal structure is a cubic structure with four-fold symmetry. It has a total of 16 symmetry operations, including four-fold rotations, three-fold rotations, and mirror planes. The unit cell of a m3m or C4 crystal structure is a cube, and the atoms or molecules in the cell are arranged in a regular pattern.\\n \\n\\n Here's a simple representation of a m3m or C4 crystal structure:\\n \\n\\n O - O - O - O\\n |  |  |  |\\n O - O - O - O\\n |  |  |  |\\n O - O - O - O\\n |  |  |  |\\n O - O - O - O\\n \\n\\n In this representation, the O symbols represent the atoms or molecules in the crystal structure. The lines represent the bonds between the atoms, and the arrows indicate the direction of the bonds. The four-fold symmetry of the structure is evident from the fact that the arrangement of atoms and bonds is the same in all directions.\\n \\n\\n I hope this description helps you visualize the m3m or C4 crystal structure.\",\n",
       "   'role': 'assistant'}],\n",
       " 'chosen_rating': 2.8,\n",
       " 'rejected': [{'content': 'What is the point group of m3m? any crystal group get this symmetry? Since it is a cubic point group, referred to Oh? is C4 also a cubic point group?\\n<extra_id_1>Assistant\\nThe point group of m3m is m3m, which has order 16 and is a cubic point group. The crystal structure of m3m is cubic, and the symmetry of the m3m group is referred to as Oh. The point group C4 also has order 16 and is a cubic point group, and has a cubic crystal structure with symmetry referred to as Ci.\\n<extra_id_1>User\\nare m3m and C4 describing the same structure?\\n<extra_id_1>Assistant\\nYes, m3m and C4 describe the same crystal structure. This structure has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.\\n<extra_id_1>User\\nGenerate a picture for me ?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Here is a diagram of the m3m crystal structure:\\n \\n\\n ```latex\\n \\\\begin{array}{c|ccc}\\n \\\\hline\\n & 2 & 3 & m \\\\\\\\\\n \\\\hline\\n 2 & 1 & {}^{e_{2}} & {}^{e_{1}e_{2}} \\\\\\\\\\n 3 & {}^{e_{2}} & 1 & {}^{e_{1}} \\\\\\\\\\n m & {}^{e_{1}e_{2}} & {}^{e_{1}} & 1 \\\\\\\\\\n \\\\hline\\n \\\\end{array}\\n ```\\n \\n\\n The order of the point group is 16, and it has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.',\n",
       "   'role': 'assistant'}],\n",
       " 'rejected_rating': 1.6}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if there is more than one request-response pair\n",
    "for example in dataset[\"train\"]:\n",
    "    if len(example[\"chosen\"]) > 2:\n",
    "        print(example[\"chosen\"])\n",
    "        break\n",
    "    if len(example[\"rejected\"]) > 2:\n",
    "        print(example[\"rejected\"])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_example(example):\n",
    "    chosen_text = \" \".join([msg[\"content\"] for msg in example[\"chosen\"]])\n",
    "    rejected_text = \" \".join([msg[\"content\"] for msg in example[\"rejected\"]])\n",
    "    margin = example[\"chosen_rating\"] - example[\"rejected_rating\"]\n",
    "    \n",
    "    return {\"chosen\": chosen_text, \"rejected\": rejected_text, \"margin\": margin}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_dataset = dataset.map(process_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What is the point group of m3m? any crystal group get this symmetry? Since it is a cubic point group, referred to Oh? is C4 also a cubic point group?\\n<extra_id_1>Assistant\\nThe point group of m3m is m3m, which has order 16 and is a cubic point group. The crystal structure of m3m is cubic, and the symmetry of the m3m group is referred to as Oh. The point group C4 also has order 16 and is a cubic point group, and has a cubic crystal structure with symmetry referred to as Ci.\\n<extra_id_1>User\\nare m3m and C4 describing the same structure?\\n<extra_id_1>Assistant\\nYes, m3m and C4 describe the same crystal structure. This structure has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.\\n<extra_id_1>User\\nGenerate a picture for me ?',\n",
       " 'chosen': \"What is the point group of m3m? any crystal group get this symmetry? Since it is a cubic point group, referred to Oh? is C4 also a cubic point group?\\n<extra_id_1>Assistant\\nThe point group of m3m is m3m, which has order 16 and is a cubic point group. The crystal structure of m3m is cubic, and the symmetry of the m3m group is referred to as Oh. The point group C4 also has order 16 and is a cubic point group, and has a cubic crystal structure with symmetry referred to as Ci.\\n<extra_id_1>User\\nare m3m and C4 describing the same structure?\\n<extra_id_1>Assistant\\nYes, m3m and C4 describe the same crystal structure. This structure has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.\\n<extra_id_1>User\\nGenerate a picture for me ? I'm sorry, as an AI language model, I don't have the capability to generate pictures or images. However, I can provide you with a description of what a m3m or C4 crystal structure looks like:\\n \\n\\n m3m or C4 crystal structure is a cubic structure with four-fold symmetry. It has a total of 16 symmetry operations, including four-fold rotations, three-fold rotations, and mirror planes. The unit cell of a m3m or C4 crystal structure is a cube, and the atoms or molecules in the cell are arranged in a regular pattern.\\n \\n\\n Here's a simple representation of a m3m or C4 crystal structure:\\n \\n\\n O - O - O - O\\n |  |  |  |\\n O - O - O - O\\n |  |  |  |\\n O - O - O - O\\n |  |  |  |\\n O - O - O - O\\n \\n\\n In this representation, the O symbols represent the atoms or molecules in the crystal structure. The lines represent the bonds between the atoms, and the arrows indicate the direction of the bonds. The four-fold symmetry of the structure is evident from the fact that the arrangement of atoms and bonds is the same in all directions.\\n \\n\\n I hope this description helps you visualize the m3m or C4 crystal structure.\",\n",
       " 'chosen_rating': 2.8,\n",
       " 'rejected': 'What is the point group of m3m? any crystal group get this symmetry? Since it is a cubic point group, referred to Oh? is C4 also a cubic point group?\\n<extra_id_1>Assistant\\nThe point group of m3m is m3m, which has order 16 and is a cubic point group. The crystal structure of m3m is cubic, and the symmetry of the m3m group is referred to as Oh. The point group C4 also has order 16 and is a cubic point group, and has a cubic crystal structure with symmetry referred to as Ci.\\n<extra_id_1>User\\nare m3m and C4 describing the same structure?\\n<extra_id_1>Assistant\\nYes, m3m and C4 describe the same crystal structure. This structure has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.\\n<extra_id_1>User\\nGenerate a picture for me ? Here is a diagram of the m3m crystal structure:\\n \\n\\n ```latex\\n \\\\begin{array}{c|ccc}\\n \\\\hline\\n & 2 & 3 & m \\\\\\\\\\n \\\\hline\\n 2 & 1 & {}^{e_{2}} & {}^{e_{1}e_{2}} \\\\\\\\\\n 3 & {}^{e_{2}} & 1 & {}^{e_{1}} \\\\\\\\\\n m & {}^{e_{1}e_{2}} & {}^{e_{1}} & 1 \\\\\\\\\\n \\\\hline\\n \\\\end{array}\\n ```\\n \\n\\n The order of the point group is 16, and it has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.',\n",
       " 'rejected_rating': 1.6,\n",
       " 'margin': 1.1999999999999997}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_example(example):\n",
    "    def tokenize_text(text):\n",
    "        return tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            padding=\"max_length\"\n",
    "        )\n",
    "    \n",
    "    tokenized_chosen = tokenize_text(example[\"chosen\"])\n",
    "    tokenized_rejected = tokenize_text(example[\"rejected\"])\n",
    "\n",
    "    return {\n",
    "        \"input_ids_chosen\": tokenized_chosen[\"input_ids\"],\n",
    "        \"attention_mask_chosen\": tokenized_chosen[\"attention_mask\"],\n",
    "        \"input_ids_rejected\": tokenized_rejected[\"input_ids\"],\n",
    "        \"attention_mask_rejected\": tokenized_rejected[\"attention_mask\"],\n",
    "        \"margin\": example[\"margin\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = processed_dataset.map(tokenize_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'chosen_rating', 'rejected', 'rejected_rating', 'margin', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "        num_rows: 6942\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'chosen_rating', 'rejected', 'rejected_rating', 'margin', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "        num_rows: 1736\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'What is the point group of m3m? any crystal group get this symmetry? Since it is a cubic point group, referred to Oh? is C4 also a cubic point group?\\n<extra_id_1>Assistant\\nThe point group of m3m is m3m, which has order 16 and is a cubic point group. The crystal structure of m3m is cubic, and the symmetry of the m3m group is referred to as Oh. The point group C4 also has order 16 and is a cubic point group, and has a cubic crystal structure with symmetry referred to as Ci.\\n<extra_id_1>User\\nare m3m and C4 describing the same structure?\\n<extra_id_1>Assistant\\nYes, m3m and C4 describe the same crystal structure. This structure has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.\\n<extra_id_1>User\\nGenerate a picture for me ?',\n",
       " 'chosen': \"What is the point group of m3m? any crystal group get this symmetry? Since it is a cubic point group, referred to Oh? is C4 also a cubic point group?\\n<extra_id_1>Assistant\\nThe point group of m3m is m3m, which has order 16 and is a cubic point group. The crystal structure of m3m is cubic, and the symmetry of the m3m group is referred to as Oh. The point group C4 also has order 16 and is a cubic point group, and has a cubic crystal structure with symmetry referred to as Ci.\\n<extra_id_1>User\\nare m3m and C4 describing the same structure?\\n<extra_id_1>Assistant\\nYes, m3m and C4 describe the same crystal structure. This structure has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.\\n<extra_id_1>User\\nGenerate a picture for me ? I'm sorry, as an AI language model, I don't have the capability to generate pictures or images. However, I can provide you with a description of what a m3m or C4 crystal structure looks like:\\n \\n\\n m3m or C4 crystal structure is a cubic structure with four-fold symmetry. It has a total of 16 symmetry operations, including four-fold rotations, three-fold rotations, and mirror planes. The unit cell of a m3m or C4 crystal structure is a cube, and the atoms or molecules in the cell are arranged in a regular pattern.\\n \\n\\n Here's a simple representation of a m3m or C4 crystal structure:\\n \\n\\n O - O - O - O\\n |  |  |  |\\n O - O - O - O\\n |  |  |  |\\n O - O - O - O\\n |  |  |  |\\n O - O - O - O\\n \\n\\n In this representation, the O symbols represent the atoms or molecules in the crystal structure. The lines represent the bonds between the atoms, and the arrows indicate the direction of the bonds. The four-fold symmetry of the structure is evident from the fact that the arrangement of atoms and bonds is the same in all directions.\\n \\n\\n I hope this description helps you visualize the m3m or C4 crystal structure.\",\n",
       " 'chosen_rating': 2.8,\n",
       " 'rejected': 'What is the point group of m3m? any crystal group get this symmetry? Since it is a cubic point group, referred to Oh? is C4 also a cubic point group?\\n<extra_id_1>Assistant\\nThe point group of m3m is m3m, which has order 16 and is a cubic point group. The crystal structure of m3m is cubic, and the symmetry of the m3m group is referred to as Oh. The point group C4 also has order 16 and is a cubic point group, and has a cubic crystal structure with symmetry referred to as Ci.\\n<extra_id_1>User\\nare m3m and C4 describing the same structure?\\n<extra_id_1>Assistant\\nYes, m3m and C4 describe the same crystal structure. This structure has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.\\n<extra_id_1>User\\nGenerate a picture for me ? Here is a diagram of the m3m crystal structure:\\n \\n\\n ```latex\\n \\\\begin{array}{c|ccc}\\n \\\\hline\\n & 2 & 3 & m \\\\\\\\\\n \\\\hline\\n 2 & 1 & {}^{e_{2}} & {}^{e_{1}e_{2}} \\\\\\\\\\n 3 & {}^{e_{2}} & 1 & {}^{e_{1}} \\\\\\\\\\n m & {}^{e_{1}e_{2}} & {}^{e_{1}} & 1 \\\\\\\\\\n \\\\hline\\n \\\\end{array}\\n ```\\n \\n\\n The order of the point group is 16, and it has a four-fold rotation axis along the center of the cube, and eight mirror planes along the diagonal directions of the cube.',\n",
       " 'rejected_rating': 1.6,\n",
       " 'margin': 1.1999999999999997,\n",
       " 'input_ids_chosen': [1780,\n",
       "  314,\n",
       "  260,\n",
       "  1225,\n",
       "  1528,\n",
       "  282,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  47,\n",
       "  750,\n",
       "  13253,\n",
       "  1528,\n",
       "  820,\n",
       "  451,\n",
       "  19536,\n",
       "  47,\n",
       "  4311,\n",
       "  357,\n",
       "  314,\n",
       "  253,\n",
       "  16992,\n",
       "  1225,\n",
       "  1528,\n",
       "  28,\n",
       "  5124,\n",
       "  288,\n",
       "  6910,\n",
       "  47,\n",
       "  314,\n",
       "  340,\n",
       "  36,\n",
       "  597,\n",
       "  253,\n",
       "  16992,\n",
       "  1225,\n",
       "  1528,\n",
       "  47,\n",
       "  198,\n",
       "  44,\n",
       "  20082,\n",
       "  79,\n",
       "  311,\n",
       "  79,\n",
       "  33,\n",
       "  46,\n",
       "  9519,\n",
       "  9531,\n",
       "  198,\n",
       "  504,\n",
       "  1225,\n",
       "  1528,\n",
       "  282,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  314,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  28,\n",
       "  527,\n",
       "  553,\n",
       "  1686,\n",
       "  216,\n",
       "  33,\n",
       "  38,\n",
       "  284,\n",
       "  314,\n",
       "  253,\n",
       "  16992,\n",
       "  1225,\n",
       "  1528,\n",
       "  30,\n",
       "  378,\n",
       "  13253,\n",
       "  2678,\n",
       "  282,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  314,\n",
       "  16992,\n",
       "  28,\n",
       "  284,\n",
       "  260,\n",
       "  19536,\n",
       "  282,\n",
       "  260,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  1528,\n",
       "  314,\n",
       "  5124,\n",
       "  288,\n",
       "  347,\n",
       "  6910,\n",
       "  30,\n",
       "  378,\n",
       "  1225,\n",
       "  1528,\n",
       "  340,\n",
       "  36,\n",
       "  597,\n",
       "  553,\n",
       "  1686,\n",
       "  216,\n",
       "  33,\n",
       "  38,\n",
       "  284,\n",
       "  314,\n",
       "  253,\n",
       "  16992,\n",
       "  1225,\n",
       "  1528,\n",
       "  28,\n",
       "  284,\n",
       "  553,\n",
       "  253,\n",
       "  16992,\n",
       "  13253,\n",
       "  2678,\n",
       "  351,\n",
       "  19536,\n",
       "  5124,\n",
       "  288,\n",
       "  347,\n",
       "  42819,\n",
       "  30,\n",
       "  198,\n",
       "  44,\n",
       "  20082,\n",
       "  79,\n",
       "  311,\n",
       "  79,\n",
       "  33,\n",
       "  46,\n",
       "  11126,\n",
       "  198,\n",
       "  522,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  284,\n",
       "  340,\n",
       "  36,\n",
       "  10131,\n",
       "  260,\n",
       "  1142,\n",
       "  2678,\n",
       "  47,\n",
       "  198,\n",
       "  44,\n",
       "  20082,\n",
       "  79,\n",
       "  311,\n",
       "  79,\n",
       "  33,\n",
       "  46,\n",
       "  9519,\n",
       "  9531,\n",
       "  198,\n",
       "  10539,\n",
       "  28,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  284,\n",
       "  340,\n",
       "  36,\n",
       "  5125,\n",
       "  260,\n",
       "  1142,\n",
       "  13253,\n",
       "  2678,\n",
       "  30,\n",
       "  669,\n",
       "  2678,\n",
       "  553,\n",
       "  253,\n",
       "  1876,\n",
       "  29,\n",
       "  7177,\n",
       "  10792,\n",
       "  6867,\n",
       "  1683,\n",
       "  260,\n",
       "  3712,\n",
       "  282,\n",
       "  260,\n",
       "  20636,\n",
       "  28,\n",
       "  284,\n",
       "  4475,\n",
       "  11558,\n",
       "  13958,\n",
       "  1683,\n",
       "  260,\n",
       "  24570,\n",
       "  8939,\n",
       "  282,\n",
       "  260,\n",
       "  20636,\n",
       "  30,\n",
       "  198,\n",
       "  44,\n",
       "  20082,\n",
       "  79,\n",
       "  311,\n",
       "  79,\n",
       "  33,\n",
       "  46,\n",
       "  11126,\n",
       "  198,\n",
       "  46453,\n",
       "  253,\n",
       "  4177,\n",
       "  327,\n",
       "  549,\n",
       "  9148,\n",
       "  339,\n",
       "  5248,\n",
       "  22657,\n",
       "  28,\n",
       "  347,\n",
       "  354,\n",
       "  5646,\n",
       "  1789,\n",
       "  1743,\n",
       "  28,\n",
       "  339,\n",
       "  1326,\n",
       "  982,\n",
       "  457,\n",
       "  260,\n",
       "  12678,\n",
       "  288,\n",
       "  5051,\n",
       "  5727,\n",
       "  355,\n",
       "  3265,\n",
       "  30,\n",
       "  1423,\n",
       "  28,\n",
       "  339,\n",
       "  416,\n",
       "  1538,\n",
       "  346,\n",
       "  351,\n",
       "  253,\n",
       "  5410,\n",
       "  282,\n",
       "  732,\n",
       "  253,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  355,\n",
       "  340,\n",
       "  36,\n",
       "  13253,\n",
       "  2678,\n",
       "  5117,\n",
       "  702,\n",
       "  42,\n",
       "  3805,\n",
       "  1116,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  355,\n",
       "  340,\n",
       "  36,\n",
       "  13253,\n",
       "  2678,\n",
       "  314,\n",
       "  253,\n",
       "  16992,\n",
       "  2678,\n",
       "  351,\n",
       "  1876,\n",
       "  29,\n",
       "  7177,\n",
       "  19536,\n",
       "  30,\n",
       "  657,\n",
       "  553,\n",
       "  253,\n",
       "  2719,\n",
       "  282,\n",
       "  216,\n",
       "  33,\n",
       "  38,\n",
       "  19536,\n",
       "  4261,\n",
       "  28,\n",
       "  1285,\n",
       "  1876,\n",
       "  29,\n",
       "  7177,\n",
       "  35457,\n",
       "  28,\n",
       "  1296,\n",
       "  29,\n",
       "  7177,\n",
       "  35457,\n",
       "  28,\n",
       "  284,\n",
       "  11558,\n",
       "  13958,\n",
       "  30,\n",
       "  378,\n",
       "  2723,\n",
       "  1297,\n",
       "  282,\n",
       "  253,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  355,\n",
       "  340,\n",
       "  36,\n",
       "  13253,\n",
       "  2678,\n",
       "  314,\n",
       "  253,\n",
       "  20636,\n",
       "  28,\n",
       "  284,\n",
       "  260,\n",
       "  9288,\n",
       "  355,\n",
       "  6756,\n",
       "  281,\n",
       "  260,\n",
       "  1297,\n",
       "  359,\n",
       "  11277,\n",
       "  281,\n",
       "  253,\n",
       "  2649,\n",
       "  2191,\n",
       "  30,\n",
       "  3805,\n",
       "  1116,\n",
       "  3726,\n",
       "  506,\n",
       "  253,\n",
       "  2232,\n",
       "  6133,\n",
       "  282,\n",
       "  253,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  355,\n",
       "  340,\n",
       "  36,\n",
       "  13253,\n",
       "  2678,\n",
       "  42,\n",
       "  3805,\n",
       "  1116,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  198,\n",
       "  2504,\n",
       "  216,\n",
       "  2504,\n",
       "  216,\n",
       "  2504,\n",
       "  216,\n",
       "  2504,\n",
       "  198,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  198,\n",
       "  2504,\n",
       "  216,\n",
       "  2504,\n",
       "  216,\n",
       "  2504,\n",
       "  216,\n",
       "  2504,\n",
       "  198,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  198,\n",
       "  2504,\n",
       "  216,\n",
       "  2504,\n",
       "  216,\n",
       "  2504,\n",
       "  216,\n",
       "  2504,\n",
       "  198,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  731,\n",
       "  492,\n",
       "  3805,\n",
       "  1116,\n",
       "  533,\n",
       "  451,\n",
       "  6133,\n",
       "  28,\n",
       "  260,\n",
       "  492,\n",
       "  7349,\n",
       "  1880,\n",
       "  260,\n",
       "  9288,\n",
       "  355,\n",
       "  6756,\n",
       "  281,\n",
       "  260,\n",
       "  13253,\n",
       "  2678,\n",
       "  30,\n",
       "  378,\n",
       "  3204,\n",
       "  1880,\n",
       "  260,\n",
       "  7967,\n",
       "  826,\n",
       "  260,\n",
       "  9288,\n",
       "  28,\n",
       "  284,\n",
       "  260,\n",
       "  23096,\n",
       "  5521,\n",
       "  260,\n",
       "  4376,\n",
       "  282,\n",
       "  260,\n",
       "  7967,\n",
       "  30,\n",
       "  378,\n",
       "  1876,\n",
       "  29,\n",
       "  7177,\n",
       "  19536,\n",
       "  282,\n",
       "  260,\n",
       "  2678,\n",
       "  314,\n",
       "  9350,\n",
       "  429,\n",
       "  260,\n",
       "  1027,\n",
       "  338,\n",
       "  260,\n",
       "  11955,\n",
       "  282,\n",
       "  9288,\n",
       "  284,\n",
       "  7967,\n",
       "  314,\n",
       "  260,\n",
       "  1142,\n",
       "  281,\n",
       "  511,\n",
       "  8939,\n",
       "  30,\n",
       "  3805,\n",
       "  1116,\n",
       "  339,\n",
       "  3826,\n",
       "  451,\n",
       "  5410,\n",
       "  2311,\n",
       "  346,\n",
       "  14556,\n",
       "  260,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  355,\n",
       "  340,\n",
       "  36,\n",
       "  13253,\n",
       "  2678,\n",
       "  30,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " 'attention_mask_chosen': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'input_ids_rejected': [1780,\n",
       "  314,\n",
       "  260,\n",
       "  1225,\n",
       "  1528,\n",
       "  282,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  47,\n",
       "  750,\n",
       "  13253,\n",
       "  1528,\n",
       "  820,\n",
       "  451,\n",
       "  19536,\n",
       "  47,\n",
       "  4311,\n",
       "  357,\n",
       "  314,\n",
       "  253,\n",
       "  16992,\n",
       "  1225,\n",
       "  1528,\n",
       "  28,\n",
       "  5124,\n",
       "  288,\n",
       "  6910,\n",
       "  47,\n",
       "  314,\n",
       "  340,\n",
       "  36,\n",
       "  597,\n",
       "  253,\n",
       "  16992,\n",
       "  1225,\n",
       "  1528,\n",
       "  47,\n",
       "  198,\n",
       "  44,\n",
       "  20082,\n",
       "  79,\n",
       "  311,\n",
       "  79,\n",
       "  33,\n",
       "  46,\n",
       "  9519,\n",
       "  9531,\n",
       "  198,\n",
       "  504,\n",
       "  1225,\n",
       "  1528,\n",
       "  282,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  314,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  28,\n",
       "  527,\n",
       "  553,\n",
       "  1686,\n",
       "  216,\n",
       "  33,\n",
       "  38,\n",
       "  284,\n",
       "  314,\n",
       "  253,\n",
       "  16992,\n",
       "  1225,\n",
       "  1528,\n",
       "  30,\n",
       "  378,\n",
       "  13253,\n",
       "  2678,\n",
       "  282,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  314,\n",
       "  16992,\n",
       "  28,\n",
       "  284,\n",
       "  260,\n",
       "  19536,\n",
       "  282,\n",
       "  260,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  1528,\n",
       "  314,\n",
       "  5124,\n",
       "  288,\n",
       "  347,\n",
       "  6910,\n",
       "  30,\n",
       "  378,\n",
       "  1225,\n",
       "  1528,\n",
       "  340,\n",
       "  36,\n",
       "  597,\n",
       "  553,\n",
       "  1686,\n",
       "  216,\n",
       "  33,\n",
       "  38,\n",
       "  284,\n",
       "  314,\n",
       "  253,\n",
       "  16992,\n",
       "  1225,\n",
       "  1528,\n",
       "  28,\n",
       "  284,\n",
       "  553,\n",
       "  253,\n",
       "  16992,\n",
       "  13253,\n",
       "  2678,\n",
       "  351,\n",
       "  19536,\n",
       "  5124,\n",
       "  288,\n",
       "  347,\n",
       "  42819,\n",
       "  30,\n",
       "  198,\n",
       "  44,\n",
       "  20082,\n",
       "  79,\n",
       "  311,\n",
       "  79,\n",
       "  33,\n",
       "  46,\n",
       "  11126,\n",
       "  198,\n",
       "  522,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  284,\n",
       "  340,\n",
       "  36,\n",
       "  10131,\n",
       "  260,\n",
       "  1142,\n",
       "  2678,\n",
       "  47,\n",
       "  198,\n",
       "  44,\n",
       "  20082,\n",
       "  79,\n",
       "  311,\n",
       "  79,\n",
       "  33,\n",
       "  46,\n",
       "  9519,\n",
       "  9531,\n",
       "  198,\n",
       "  10539,\n",
       "  28,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  284,\n",
       "  340,\n",
       "  36,\n",
       "  5125,\n",
       "  260,\n",
       "  1142,\n",
       "  13253,\n",
       "  2678,\n",
       "  30,\n",
       "  669,\n",
       "  2678,\n",
       "  553,\n",
       "  253,\n",
       "  1876,\n",
       "  29,\n",
       "  7177,\n",
       "  10792,\n",
       "  6867,\n",
       "  1683,\n",
       "  260,\n",
       "  3712,\n",
       "  282,\n",
       "  260,\n",
       "  20636,\n",
       "  28,\n",
       "  284,\n",
       "  4475,\n",
       "  11558,\n",
       "  13958,\n",
       "  1683,\n",
       "  260,\n",
       "  24570,\n",
       "  8939,\n",
       "  282,\n",
       "  260,\n",
       "  20636,\n",
       "  30,\n",
       "  198,\n",
       "  44,\n",
       "  20082,\n",
       "  79,\n",
       "  311,\n",
       "  79,\n",
       "  33,\n",
       "  46,\n",
       "  11126,\n",
       "  198,\n",
       "  46453,\n",
       "  253,\n",
       "  4177,\n",
       "  327,\n",
       "  549,\n",
       "  9148,\n",
       "  3726,\n",
       "  314,\n",
       "  253,\n",
       "  11262,\n",
       "  282,\n",
       "  260,\n",
       "  283,\n",
       "  35,\n",
       "  93,\n",
       "  13253,\n",
       "  2678,\n",
       "  42,\n",
       "  3805,\n",
       "  1116,\n",
       "  47182,\n",
       "  12837,\n",
       "  104,\n",
       "  198,\n",
       "  3814,\n",
       "  21083,\n",
       "  107,\n",
       "  4282,\n",
       "  15009,\n",
       "  83,\n",
       "  108,\n",
       "  509,\n",
       "  83,\n",
       "  109,\n",
       "  198,\n",
       "  3814,\n",
       "  88,\n",
       "  1311,\n",
       "  198,\n",
       "  1456,\n",
       "  216,\n",
       "  34,\n",
       "  1456,\n",
       "  216,\n",
       "  35,\n",
       "  1456,\n",
       "  283,\n",
       "  28372,\n",
       "  198,\n",
       "  3814,\n",
       "  88,\n",
       "  1311,\n",
       "  3805,\n",
       "  34,\n",
       "  1456,\n",
       "  216,\n",
       "  33,\n",
       "  1456,\n",
       "  4548,\n",
       "  21990,\n",
       "  85,\n",
       "  11300,\n",
       "  34,\n",
       "  17859,\n",
       "  1456,\n",
       "  4548,\n",
       "  21990,\n",
       "  85,\n",
       "  11300,\n",
       "  33,\n",
       "  109,\n",
       "  85,\n",
       "  11300,\n",
       "  34,\n",
       "  17859,\n",
       "  28372,\n",
       "  3805,\n",
       "  35,\n",
       "  1456,\n",
       "  4548,\n",
       "  21990,\n",
       "  85,\n",
       "  11300,\n",
       "  34,\n",
       "  17859,\n",
       "  1456,\n",
       "  216,\n",
       "  33,\n",
       "  1456,\n",
       "  4548,\n",
       "  21990,\n",
       "  85,\n",
       "  11300,\n",
       "  33,\n",
       "  17859,\n",
       "  28372,\n",
       "  198,\n",
       "  283,\n",
       "  1456,\n",
       "  4548,\n",
       "  21990,\n",
       "  85,\n",
       "  11300,\n",
       "  33,\n",
       "  109,\n",
       "  85,\n",
       "  11300,\n",
       "  34,\n",
       "  17859,\n",
       "  1456,\n",
       "  4548,\n",
       "  21990,\n",
       "  85,\n",
       "  11300,\n",
       "  33,\n",
       "  17859,\n",
       "  1456,\n",
       "  216,\n",
       "  33,\n",
       "  28372,\n",
       "  198,\n",
       "  3814,\n",
       "  88,\n",
       "  1311,\n",
       "  198,\n",
       "  3814,\n",
       "  486,\n",
       "  107,\n",
       "  4282,\n",
       "  109,\n",
       "  198,\n",
       "  47182,\n",
       "  3805,\n",
       "  1116,\n",
       "  378,\n",
       "  1686,\n",
       "  282,\n",
       "  260,\n",
       "  1225,\n",
       "  1528,\n",
       "  314,\n",
       "  216,\n",
       "  33,\n",
       "  38,\n",
       "  28,\n",
       "  284,\n",
       "  357,\n",
       "  553,\n",
       "  253,\n",
       "  1876,\n",
       "  29,\n",
       "  7177,\n",
       "  10792,\n",
       "  6867,\n",
       "  1683,\n",
       "  260,\n",
       "  3712,\n",
       "  282,\n",
       "  260,\n",
       "  20636,\n",
       "  28,\n",
       "  284,\n",
       "  4475,\n",
       "  11558,\n",
       "  13958,\n",
       "  1683,\n",
       "  260,\n",
       "  24570,\n",
       "  8939,\n",
       "  282,\n",
       "  260,\n",
       "  20636,\n",
       "  30,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2,\n",
       "  2],\n",
       " 'attention_mask_rejected': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_keep = [\n",
    "    \"input_ids_chosen\",\n",
    "    \"attention_mask_chosen\",\n",
    "    \"input_ids_rejected\",\n",
    "    \"attention_mask_rejected\",\n",
    "    \"margin\"\n",
    "]\n",
    "\n",
    "tokenized_dataset[\"train\"] = tokenized_dataset[\"train\"].remove_columns(\n",
    "    [col for col in tokenized_dataset[\"train\"].column_names if col not in columns_to_keep]\n",
    ")\n",
    "\n",
    "tokenized_dataset[\"test\"] = tokenized_dataset[\"test\"].remove_columns(\n",
    "    [col for col in tokenized_dataset[\"test\"].column_names if col not in columns_to_keep]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['margin', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "        num_rows: 6942\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['margin', 'input_ids_chosen', 'attention_mask_chosen', 'input_ids_rejected', 'attention_mask_rejected'],\n",
       "        num_rows: 1736\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Reward model using LoRa and RewardTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at HuggingFaceTB/SmolLM2-135M-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='868' max='868' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [868/868 11:04, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.431500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.282300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.295200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.245300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.348800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.228900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.219300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.222300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.236400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.278100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.244200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.224000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.250700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=868, training_loss=1.2665826801880165, metrics={'train_runtime': 665.7197, 'train_samples_per_second': 10.428, 'train_steps_per_second': 1.304, 'total_flos': 0.0, 'train_loss': 1.2665826801880165, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, TaskType\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=1).to(device)\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    inference_mode=False,\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    ")\n",
    "\n",
    "training_args = RewardConfig(\n",
    "    output_dir=\"./RM_output\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=2,\n",
    "    eval_accumulation_steps=2,\n",
    "    learning_rate=5e-5,\n",
    "    max_length=512,\n",
    "    fp16=True,\n",
    "    logging_steps=50,\n",
    "    eval_steps=1000,\n",
    "    center_rewards_coefficient=0.01,  # encouraging the model to produce mean-zero outputs\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    processing_class=tokenizer,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
