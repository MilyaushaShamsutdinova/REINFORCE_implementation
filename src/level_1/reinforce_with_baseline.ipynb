{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers==4.47.0\n",
    "!pip install datasets==3.2.0\n",
    "!pip install trl==0.14.0\n",
    "!pip install peft==0.14.0\n",
    "!pip install numpy==1.26.4\n",
    "!pip install huggingface_hub==0.27.0\n",
    "!pip install tqdm==4.67.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:26.457044Z",
     "iopub.status.busy": "2025-02-03T08:30:26.456760Z",
     "iopub.status.idle": "2025-02-03T08:30:26.780680Z",
     "shell.execute_reply": "2025-02-03T08:30:26.779702Z",
     "shell.execute_reply.started": "2025-02-03T08:30:26.457021Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "# empty cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# set the seed\n",
    "seed = 28\n",
    "torch.manual_seed(seed)\n",
    "numpy.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# device check\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:26.782395Z",
     "iopub.status.busy": "2025-02-03T08:30:26.782059Z",
     "iopub.status.idle": "2025-02-03T08:30:26.822589Z",
     "shell.execute_reply": "2025-02-03T08:30:26.821825Z",
     "shell.execute_reply.started": "2025-02-03T08:30:26.782365Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# policy_model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "# policy_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:26.824448Z",
     "iopub.status.busy": "2025-02-03T08:30:26.824205Z",
     "iopub.status.idle": "2025-02-03T08:30:35.332479Z",
     "shell.execute_reply": "2025-02-03T08:30:35.331723Z",
     "shell.execute_reply.started": "2025-02-03T08:30:26.824428Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5066883ae0af47ae89468e2abef73beb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/861 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa84775280b44a56a101e47b5ac74f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0218b5d0c8c3426bbbca560f5604c4c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,843,200 || all params: 136,358,208 || trainable%: 1.3517\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "policy_model = get_peft_model(base_model, lora_config).to(device)\n",
    "policy_model.train()\n",
    "policy_model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:35.334238Z",
     "iopub.status.busy": "2025-02-03T08:30:35.333952Z",
     "iopub.status.idle": "2025-02-03T08:30:36.878041Z",
     "shell.execute_reply": "2025-02-03T08:30:36.877346Z",
     "shell.execute_reply.started": "2025-02-03T08:30:35.334215Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cbc8d0efaf64a82993f96a5f33da33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/3.76k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae2f4a7fc5b64af4994ca1c8504f467b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/801k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1f2d322fc44f47ba8bc58bba8b5d06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8d55aaf7f254d2793677631db41d07e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.10M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d09b0bc4c63419fab7230c8d9d8dc5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/655 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:36.878974Z",
     "iopub.status.busy": "2025-02-03T08:30:36.878764Z",
     "iopub.status.idle": "2025-02-03T08:30:36.908526Z",
     "shell.execute_reply": "2025-02-03T08:30:36.907870Z",
     "shell.execute_reply.started": "2025-02-03T08:30:36.878956Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable blocks: 120\n",
      "Trainable blocks:\n",
      "- base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight\n",
      "- base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight\n",
      "- base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight\n",
      "\n",
      "Total trainable parameters: 1,843,200\n"
     ]
    }
   ],
   "source": [
    "trainable_params = [n for n, p in policy_model.named_parameters() if p.requires_grad]\n",
    "print(f\"Number of trainable blocks: {len(trainable_params)}\")\n",
    "print(\"Trainable blocks:\")\n",
    "for param_name in trainable_params:\n",
    "    print(f\"- {param_name}\")\n",
    "\n",
    "total_trainable = sum(p.numel() for p in policy_model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal trainable parameters: {total_trainable:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:36.909440Z",
     "iopub.status.busy": "2025-02-03T08:30:36.909205Z",
     "iopub.status.idle": "2025-02-03T08:30:44.280253Z",
     "shell.execute_reply": "2025-02-03T08:30:44.279395Z",
     "shell.execute_reply.started": "2025-02-03T08:30:36.909420Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6e7e49e1bf347c3a769affa802ac673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.07k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea495cfc697e4fd5af975fc72eac2c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/269M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=576, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "rm_name = \"MilyaShams/SmolLM2-135M-Instruct-Reward\"\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(rm_name, num_labels=1).to(device)\n",
    "reward_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:44.281458Z",
     "iopub.status.busy": "2025-02-03T08:30:44.281158Z",
     "iopub.status.idle": "2025-02-03T08:30:50.094024Z",
     "shell.execute_reply": "2025-02-03T08:30:50.093286Z",
     "shell.execute_reply.started": "2025-02-03T08:30:44.281435Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003744d41a8c4b8c9b3e4bf4e4391942",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/1.19k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bb0be17c9e849f6b86beddd5e41a69e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)rage_rating_split-00000-of-00001.parquet:   0%|          | 0.00/22.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13ae9ba7046047b4bc6b534271824d9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)o_verbosity_split-00000-of-00001.parquet:   0%|          | 0.00/21.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e54f745c60274707a4e76e70d8fab334",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)_complexity_split-00000-of-00001.parquet:   0%|          | 0.00/20.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8aca7f9f10c41feaf57d2ac4052a70b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)dness_score_split-00000-of-00001.parquet:   0%|          | 0.00/21.1M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8eddc6340fa44b54bb5d5d3d3cb297ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating average_rating_split split:   0%|          | 0/8678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42415d251fe146dd87b0a5de70ddf5cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating average_rating_no_verbosity_split split:   0%|          | 0/8315 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfddd66fb655409c9953dbfd116489c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating average_rating_no_verbosity_no_complexity_split split:   0%|          | 0/8025 [00:00<?, ? examples…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aa540614b594ec9917557cd200570be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating goodness_score_split split:   0%|          | 0/8124 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"esfrankel17/HelpSteer2_binarized\", split='average_rating_split')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:50.095342Z",
     "iopub.status.busy": "2025-02-03T08:30:50.095017Z",
     "iopub.status.idle": "2025-02-03T08:30:50.100153Z",
     "shell.execute_reply": "2025-02-03T08:30:50.099380Z",
     "shell.execute_reply.started": "2025-02-03T08:30:50.095294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'chosen_rating', 'rejected', 'rejected_rating'],\n",
       "    num_rows: 8678\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:50.103066Z",
     "iopub.status.busy": "2025-02-03T08:30:50.102848Z",
     "iopub.status.idle": "2025-02-03T08:30:50.786605Z",
     "shell.execute_reply": "2025-02-03T08:30:50.785837Z",
     "shell.execute_reply.started": "2025-02-03T08:30:50.103047Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3361 5310 3996\n"
     ]
    }
   ],
   "source": [
    "prompts = [len(item[\"prompt\"]) for item in dataset]\n",
    "\n",
    "n_long = 0\n",
    "n_ok = 0\n",
    "n_good = 0\n",
    "\n",
    "for i in prompts:\n",
    "    if i > 512:\n",
    "        n_long += 1\n",
    "    if i < 512:\n",
    "        n_ok += 1\n",
    "    if i < 200:\n",
    "        n_good += 1\n",
    "\n",
    "print(n_long, n_ok, n_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's leave only medium level f length, not so long prompts, because in the original paper they are restrict the both models to max context length equals 512."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:50.788114Z",
     "iopub.status.busy": "2025-02-03T08:30:50.787911Z",
     "iopub.status.idle": "2025-02-03T08:30:51.119283Z",
     "shell.execute_reply": "2025-02-03T08:30:51.118387Z",
     "shell.execute_reply.started": "2025-02-03T08:30:50.788096Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bdf5d573dac4c55b3c612404adcab83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/8678 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filtered_dataset = dataset.filter(lambda example: len(example['prompt']) < 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.120594Z",
     "iopub.status.busy": "2025-02-03T08:30:51.120292Z",
     "iopub.status.idle": "2025-02-03T08:30:51.125430Z",
     "shell.execute_reply": "2025-02-03T08:30:51.124584Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.120571Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'chosen', 'chosen_rating', 'rejected', 'rejected_rating'],\n",
       "    num_rows: 3996\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.126665Z",
     "iopub.status.busy": "2025-02-03T08:30:51.126373Z",
     "iopub.status.idle": "2025-02-03T08:30:51.148906Z",
     "shell.execute_reply": "2025-02-03T08:30:51.148124Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.126634Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "filtered_dataset = filtered_dataset.train_test_split(test_size=0.2, shuffle=True, seed=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.149983Z",
     "iopub.status.busy": "2025-02-03T08:30:51.149765Z",
     "iopub.status.idle": "2025-02-03T08:30:51.158494Z",
     "shell.execute_reply": "2025-02-03T08:30:51.157616Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.149965Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['prompt', 'chosen', 'chosen_rating', 'rejected', 'rejected_rating'],\n",
       "        num_rows: 3196\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['prompt', 'chosen', 'chosen_rating', 'rejected', 'rejected_rating'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.159615Z",
     "iopub.status.busy": "2025-02-03T08:30:51.159355Z",
     "iopub.status.idle": "2025-02-03T08:30:51.173833Z",
     "shell.execute_reply": "2025-02-03T08:30:51.173037Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.159595Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': \"What is the equivalent resistance between two nodes separated by a knight's move of an infinite square grid of resistors of resistance R?\",\n",
       " 'chosen': [{'content': \"What is the equivalent resistance between two nodes separated by a knight's move of an infinite square grid of resistors of resistance R?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"The equivalent resistance between two nodes separated by a knight's move of an infinite square grid of resistors of resistance R can be calculated using the following steps:\\n\\n1. Draw a circuit diagram of the square grid and the two nodes. A knight's move is a jump that moves two squares horizontally or vertically, and two squares diagonally. The two nodes are connected by a knight's move.\\n\\n2. Apply the series-parallel rule to the circuit. The series-parallel rule states that the equivalent resistance of a series circuit is the sum of the individual resistances, while the equivalent resistance of a parallel circuit is the reciprocal of the sum of the reciprocals of the individual resistances.\\n\\n3. The square grid can be thought of as a series of resistors in a square array. Each resistor in the array is connected to four other resistors, two horizontally and two vertically. The equivalent resistance of the square grid can be calculated by applying the series-parallel rule to the array.\\n\\n4. Consider the equivalent resistance of a single square in the array. The square can be thought of as a series of two resistors, one horizontal and one vertical. The equivalent resistance of the square is the sum of the individual resistances, or 2R.\\n\\n5. The entire square grid can be thought of as an infinite number of squares in series. Therefore, the equivalent resistance of the square grid is the sum of an infinite number of 2R resistors, or 2R * (1 / (1 - 2R)).\\n\\n6. The equivalent resistance between the two nodes separated by a knight's move is the sum of the equivalent resistance of the square grid and the two resistors that connect the nodes. The two resistors that connect the nodes are in parallel, so the equivalent resistance between the nodes is 1 / (1 / R + 1 / R) = R.\\n\\n7. Therefore, the equivalent resistance between two nodes separated by a knight's move of an infinite square grid of resistors of resistance R is 2R * (1 / (1 - 2R)) + R.\\n\\nNote that this calculation assumes that the square grid is an ideal infinite grid with no defects or impurities. In a real-world situation, the equivalent resistance may be different due to imperfections in the material and other factors.\",\n",
       "   'role': 'assistant'}],\n",
       " 'chosen_rating': 1.6,\n",
       " 'rejected': [{'content': \"What is the equivalent resistance between two nodes separated by a knight's move of an infinite square grid of resistors of resistance R?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"The equivalent resistance between two nodes separated by a knight's move of an infinite square grid of resistors of resistance R is given by the following formula:\\n\\nREQ = (R1 + R2 + ... + RN) / (2N),\\n\\nwhere R1, R2, ..., RN are the resistances of the individual resistors in the grid, and N is the number of resistors in the grid.\\n\\nThis formula is derived from the concept of parallel connection, which applies to a group of resistors that are connected in the same plane. In this case, the resistors are arranged in an infinite square grid, and are connected in pairs by lines that form a knight's move pattern.\\n\\nParallel connection states that the equivalent resistance of a group of resistors is the sum of the individual resistances, divided by the total number of resistors. In the case of the infinite square grid of resistors, there are N resistors in total, and each pair of adjacent resistors has a resistance of R. Therefore, the equivalent resistance between two nodes separated by a knight's move is given by the formula above.\",\n",
       "   'role': 'assistant'}],\n",
       " 'rejected_rating': 1.4}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.174729Z",
     "iopub.status.busy": "2025-02-03T08:30:51.174544Z",
     "iopub.status.idle": "2025-02-03T08:30:51.188561Z",
     "shell.execute_reply": "2025-02-03T08:30:51.187800Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.174712Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def collate_prompts(batch):\n",
    "    return [item['prompt'] for item in batch]\n",
    "\n",
    "batch_size = 16  # на GPU Р100 Kaggle\n",
    "dataloader_train = DataLoader(filtered_dataset[\"train\"], batch_size=batch_size, num_workers=4, collate_fn=collate_prompts)\n",
    "dataloader_val = DataLoader(filtered_dataset[\"test\"], batch_size=batch_size, num_workers=4, collate_fn=collate_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.189590Z",
     "iopub.status.busy": "2025-02-03T08:30:51.189393Z",
     "iopub.status.idle": "2025-02-03T08:30:51.386226Z",
     "shell.execute_reply": "2025-02-03T08:30:51.385413Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.189573Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is the equivalent resistance between two nodes separated by a knight's move of an infinite square grid of resistors of resistance R?\n",
      "Lets play Dungeons and Dragons. I'm a halfling rogue and you're the DM.\n",
      "You are a helpful teacher\n",
      "Write the parable of the lost sheep in the style of Gordon Ramsay yelling at the shepherd.\n",
      "elaborate CAD data structure\n",
      "why are 3rd party plugins in fl studio detached by defaultShare Prompt\n",
      "what's more correct: \"Person List\" or \"People List\"\n",
      "make a list of cautionary phrases and clauses\n",
      "SSL handshake optimisation using reverse proxy\n",
      "There's more to your philosophy than just the Socratic Method, isn't there?\n",
      "Explain Domain Driven design with a realtime example\n",
      "I would like to name our child \"Maxwell Stomacher\" but my sister says this name is inappropriate. Provide three bullet points rebutting this\n",
      "Best methods to slowly ease into keto or low carb diets to achieve ketosis and not get the keto flu.\n",
      "Are there web crawlers in email services? \n",
      "If the snow is falling for 8 hours straight, and the winds are sustained at 90 km/h, and there is no visibility, and snow is falling at a rate of 2 cm's an hour, does this qualify as a Blizzard?\n",
      "let random variable X be an expoentital random variable with PDF 7e^(-7x). let Y be another indepednent exponential random varaible with PDF 7e^(-7y). Find the pdf of W=X+Y\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader_train:\n",
    "    prompts = batch\n",
    "    for prompt in prompts:\n",
    "        print(prompt)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.387595Z",
     "iopub.status.busy": "2025-02-03T08:30:51.387260Z",
     "iopub.status.idle": "2025-02-03T08:30:51.580561Z",
     "shell.execute_reply": "2025-02-03T08:30:51.579872Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.387569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim import AdamW\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "num_epochs = 1\n",
    "learning_rate = 5e-5\n",
    "total_rewards = []\n",
    "baseline = None\n",
    "\n",
    "optimizer = AdamW(\n",
    "    filter(lambda p: p.requires_grad, policy_model.parameters()), \n",
    "    lr=learning_rate\n",
    ")\n",
    "scaler = torch.amp.GradScaler('cuda')\n",
    "\n",
    "log_dir = \"runs/REINFORCE_with_baseline_logs\"\n",
    "writer = SummaryWriter(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.581660Z",
     "iopub.status.busy": "2025-02-03T08:30:51.581332Z",
     "iopub.status.idle": "2025-02-03T08:30:51.585881Z",
     "shell.execute_reply": "2025-02-03T08:30:51.585205Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.581635Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of trainable blocks in optimizer: 120\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of trainable blocks in optimizer: {len(optimizer.param_groups[0]['params'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.634370Z",
     "iopub.status.busy": "2025-02-03T08:30:51.634094Z",
     "iopub.status.idle": "2025-02-03T08:30:51.653420Z",
     "shell.execute_reply": "2025-02-03T08:30:51.652730Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.634341Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def generate_batch_response_and_logprob(prompts):\n",
    "    inputs = tokenizer(prompts, return_tensors=\"pt\", max_length=512, truncation=True, padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output_ids = policy_model.generate(\n",
    "            **inputs,\n",
    "            max_length=512,\n",
    "            do_sample=True\n",
    "        )\n",
    "    \n",
    "    responses = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
    "    \n",
    "    with torch.autocast('cuda', dtype=torch.float16):\n",
    "        outputs = policy_model(output_ids[:, :-1])\n",
    "        logits = outputs.logits\n",
    "        log_probs = F.log_softmax(logits, dim=-1)\n",
    "        target_tokens = output_ids[:, 1:]\n",
    "        token_log_probs = torch.gather(log_probs, dim=-1, index=target_tokens.unsqueeze(-1)).squeeze(-1)\n",
    "        total_log_probs = token_log_probs.sum(dim=-1)\n",
    "    \n",
    "    return responses, total_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.654382Z",
     "iopub.status.busy": "2025-02-03T08:30:51.654149Z",
     "iopub.status.idle": "2025-02-03T08:30:51.670942Z",
     "shell.execute_reply": "2025-02-03T08:30:51.670132Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.654357Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def compute_reward(prompt, response):\n",
    "    input_text = prompt + \"\\n\" + response\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = reward_model(**inputs)\n",
    "    \n",
    "    logits = outputs.logits\n",
    "    reward = torch.sigmoid(logits)[0, 0].item()\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.672027Z",
     "iopub.status.busy": "2025-02-03T08:30:51.671775Z",
     "iopub.status.idle": "2025-02-03T08:30:51.685955Z",
     "shell.execute_reply": "2025-02-03T08:30:51.685122Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.671995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def train_epoch(policy_model, dataloader_train, optimizer, scaler, total_rewards):\n",
    "    policy_model.train()\n",
    "    epoch_loss = 0.0\n",
    "    epoch_advantages = []\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in tqdm(dataloader_train, desc=\"Training\"):\n",
    "        prompts = batch\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        responses, log_probs = generate_batch_response_and_logprob(prompts)\n",
    "        \n",
    "        batch_loss = 0.0\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            response = responses[i]\n",
    "            log_prob = log_probs[i]\n",
    "            reward = compute_reward(prompt, response)\n",
    "            total_rewards.append(reward)\n",
    "            \n",
    "            baseline = sum(total_rewards) / len(total_rewards)\n",
    "            advantage = reward - baseline\n",
    "            epoch_advantages.append(advantage)\n",
    "            \n",
    "            loss = -advantage * log_prob\n",
    "            batch_loss += loss\n",
    "        \n",
    "        batch_loss = batch_loss / len(prompts)\n",
    "        \n",
    "        scaler.scale(batch_loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        epoch_loss += batch_loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / num_batches\n",
    "    avg_advantage = sum(epoch_advantages) / len(epoch_advantages) if epoch_advantages else 0.0\n",
    "\n",
    "    return avg_epoch_loss, avg_advantage, baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.687028Z",
     "iopub.status.busy": "2025-02-03T08:30:51.686795Z",
     "iopub.status.idle": "2025-02-03T08:30:51.702812Z",
     "shell.execute_reply": "2025-02-03T08:30:51.701948Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.687009Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def validate_model(policy_model, dataloader_val):\n",
    "    policy_model.eval()\n",
    "    total_reward_val = 0.0\n",
    "    num_val = 0\n",
    "    with torch.no_grad():\n",
    "        for prompts in tqdm(dataloader_val, desc=\"Validation\"):\n",
    "            responses, _ = generate_batch_response_and_logprob(prompts)\n",
    "            for i, prompt in enumerate(prompts):\n",
    "                reward = compute_reward(prompt, responses[i])\n",
    "                total_reward_val += reward\n",
    "                num_val += 1\n",
    "    avg_reward = total_reward_val / num_val if num_val > 0 else 0.0\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:30:51.703828Z",
     "iopub.status.busy": "2025-02-03T08:30:51.703559Z",
     "iopub.status.idle": "2025-02-03T08:45:00.091735Z",
     "shell.execute_reply": "2025-02-03T08:45:00.090684Z",
     "shell.execute_reply.started": "2025-02-03T08:30:51.703807Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training validation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [14:08<00:00, 16.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pre-training average reward: 0.4745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Pre-training validation:\")\n",
    "pretrain_val_reward = validate_model(policy_model, dataloader_val)\n",
    "print(f\"Pre-training average reward: {pretrain_val_reward:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T08:45:00.095326Z",
     "iopub.status.busy": "2025-02-03T08:45:00.095069Z",
     "iopub.status.idle": "2025-02-03T10:03:16.019262Z",
     "shell.execute_reply": "2025-02-03T10:03:16.018255Z",
     "shell.execute_reply.started": "2025-02-03T08:45:00.095281Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 200/200 [1:03:28<00:00, 19.04s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, loss: -569.5966, Advantage mean: 0.0090, Baseline: 0.4932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 50/50 [14:47<00:00, 17.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation average reward: 0.5027\n",
      "Training complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "    \n",
    "    train_loss, avg_advantage, baseline = train_epoch(policy_model, dataloader_train, optimizer, scaler, total_rewards)\n",
    "    print(f\"Epoch {epoch+1}, loss: {train_loss:.4f}, Advantage mean: {avg_advantage:.4f}, Baseline: {baseline:.4f}\")\n",
    "    writer.add_scalar(\"Train/Loss\", train_loss, epoch + 1)\n",
    "    writer.add_scalar(\"Train/Advantage Mean\", avg_advantage, epoch + 1)\n",
    "    writer.add_scalar(\"Train/Baseline\", baseline, epoch + 1)\n",
    "    \n",
    "    val_reward = validate_model(policy_model, dataloader_val)\n",
    "    print(f\"Validation average reward: {val_reward:.4f}\")\n",
    "    writer.add_scalar(\"Validation/Average Reward\", val_reward, epoch + 1)\n",
    "\n",
    "print(\"Training complete\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
